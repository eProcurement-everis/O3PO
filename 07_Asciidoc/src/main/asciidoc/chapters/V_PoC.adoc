
== V. Proof of Concept

The ePO development team agreed with the WG members to provide a means to test the deliverables produced, especially
the Conceptual Data Model and the OWL-TTL implementation. With this purpose in mind a Proof of Concept was planned and
executed.

.ePO Testing
image::ePOTesting.png[ePO Testing, align="center"]

The main objectives of the Proof of Concept were:

. Test the coherence of the Conceptual Data Model (of the T-Box);
. Test the consistency of the data once loaded (in the A-Box);
. Test the effectiveness of the OWL implementation of the eProcurement Ontology (ePO); and
. Test the feasibility of the ePO to support the Use Cases defined in ePO v.1.0.

Hence a varied set of activities were planned with these objectives in mind. The diagram below
shows the activities that were planned and executed to develop the Proof of Concept:

.ePO Project v2.0.0 - Proof-Of-Concept
image::ePO_PoC.png[v2.0.0 - Proof-Of-Concept, align="center"]

The following subsections explain how each of the activities mentioned in the diagram above has been
developed and where to check the inputs, processes and results.

=== Activity 1: Use Cases
.Competency Questions, activity summary
[cols="<1,<4"]
|===
|*Activity name*:|Identify and study the Use Cases related to monitoring and transparency.
|*Responsible team*:|OP's contractor team.
|*Inputs*:|ePO v1.0 link:https://github.com/eprocurementontology/eprocurementontology/wiki/Use-case-1.-Data-journalism[Use Case 1]
and link:https://github.com/eprocurementontology/eprocurementontology/issues/11[Issue #11].
|*Outputs*:|Study of the Use Cases (slightly renaming).
|===

The ePO v1.0 focused on three different Use Cases:

* link:https://github.com/eprocurementontology/eprocurementontology/wiki/Use-case-1.-Data-journalism[Use Case 1]: Data Journalism

* link:https://github.com/eprocurementontology/eprocurementontology/wiki/Use-case-1.-Data-journalism[Use Case 2]: Automated matchmaking of procured services and products with businesses, and

* link:https://github.com/eprocurementontology/eprocurementontology/wiki/Use-case-3.-Verifying-VAT-payments-on-intracommunity-service-provision[Use Case 3]: Verifying VAT payments on intra-community service provision.

During its development a fourth Use Case was identified as relevant related to Transparency and Monitoring. This
use case was proposed through an "link:https://github.com/eprocurementontology/eprocurementontology/issues/11[Issue]",
in the GitHub repository. This Use Case was accepted as as a relevant case for transparency and monitoring.

Hence the ePO v2.0.0, which is focused only on transparency and monitoring, was developed taken into account two
Use Cases (slightly renamed):

* link:https://github.com/eprocurementontology/eprocurementontology/wiki/Use-case-1.-Transparency-and-Monitoring[Use Case 1]: Transparency and Monitoring; and

* link:https://github.com/eprocurementontology/eprocurementontology/wiki/Use--ase-4.-Analyzing-eProcurement-procedures[Use Case 4]: Analyzing eProcurement procedures.

=== Activity 2: User Stories

.User Stories, activity summary
[cols="<1,<4"]
|===
|*Activity name*:|Prepare sample (example) User Stories.
|*Responsible team*:|OP's contractor team.
|*Inputs*:|Use Cases 1 and 4.
|*Outputs*:|Examples of link:https://github.com/eprocurementontology/eprocurementontology/blob/master/v2.0.0/02_IR_DED/WayforwardCompetencyQuestions.pdf[User Stories].
|===

User Stories are a method of helping identify information requirements. The method consists in
drafting very simple sentence structured around three main questions:

. Who is the beneficiary of an action (who benefits from it)?

. What is the need?

. What is the benefit?

The structure of the sentence is always like this: “_As a <role of the user>, I need <something>in order to <benefit>._”

*Example*:

As a *contracting authority* (ROLE), I need to know *the number of tenderers* (WHAT DO I NEED?) that have submitted a tender
in order *to add it to the award notice* (BENEFIT).

Some examples of User Stories were prepared. The table below shows these sample User Stories for different
roles and related to the Use Cases 1 and 4.

.Examples of User Stories
image::UserStoriesExamplesTable.png[User Stories examples, align="center"]

=== Activity 3: Competency Questions

.Competency Questions, activity summary
[cols="<1,<4"]
|===
|*Activity name*:|Prepare sample (example) Competency Questions.
|*Responsible team*:|OP's contractor team.
|*Inputs*:|Use Cases 1 and 4, and the User Stories.
|*Outputs*:|Examples of link:https://github.com/eprocurementontology/eprocurementontology/wiki/Competency-Questions[Competency Questions].
|===

User Stories help also draft very specific questions that need to be answered in order to
meet the story. These questions will later on taken into account to draft concrete SPARQL queries.

Some examples of Competency Questions were prepared. The two tables below illustrate how these Competency
Questions, linked to their respective User Stories, may look like. The
link link:https://github.com/eprocurementontology/eprocurementontology/wiki/Competency-Questions[Competency Questions],
in the GitHub link:https://github.com/eprocurementontology/eprocurementontology/wiki[Wiki] page,
supplies a longer list of concrete examples of CQ for the WG members to get inspiration.

.Example (1/2) of Competency Questions
image::CQExample1.png[CQ example 1, align="center"]

.Examples (2/2) of User Stories
image::CQExample2.png[CQ example 2, align="center"]

=== Activity 4: Review CQs

.Revision of Competency Questions, activity summary
[cols="<1,<4"]
|===
|*Activity name*:|Review Competency Questions.
|*Responsible team*:|Working Group members.
|*Inputs*:|Competency Questions and related User Stories.
|*Outputs*:|Comments by the WG members.
|===

The examples were made available to the Working Group members through the GitHub Wiki page.
A special link:++https://github.com/eprocurementontology/eprocurementontology/issues/new?template=new_competency_question.md&labels=new%20competency%20question&title=COMPETENCY+QUESTION+-[Add a new competency question]
to add comments or create new issues related to the CQs was also made available in the
link:https://github.com/eprocurementontology/eprocurementontology/wiki/Competency-Questions[GitHub Wiki page].

=== Activity 5: Input Data Set

.Select Data Set, activity summary
[cols="<1,<4"]
|===
|*Activity name*:|Select Data Set.
|*Responsible team*:|OP's team.
|*Inputs*:|Use Cases, User Stories, Competency Questions, agreement with the members of the WG.
|*Outputs*:|Documents published on TED, accessed via the OP's link:ftp://ted.europa.eu/[FTP] server.
|===

For the extraction of data, the decision was made that the source of data should be the Notices
published on the link:http://ted.europa.eu/TED/main/HomePage.do[TED portal]. This decision was made
based on, namely, the following reasons:

* The Contract Award Notice (CAN) contains the data most relevant for Transparency, Monitoring and Procedure control (jointly
with the Contract Notice (CN));

* The CAN is the most published document, therefore the sample is richer;

* The structure and elements of the standard form for the CAN are very similar or identical to many of other
Notices. This allows to reuse a relevant part of the extraction and transformation artefacts (XSL-T) to process
many other types of Forms.

However the User Interface of the TED Portal does not allow downloading large amount of documents.
For this we used the link:ftp://ted.europa.eu/[FTP] server supplied by the OP at: ftp://ted.europa.eu/
(user: **guest**, password: **guest**).

The TED-XML specification has been evolving for the past years. Different
versions of XSD Schemas have been maintained in parallel for those years. The result is that, as
per today, different schemas are being used to express the data in alignment to the 2014 Directives.
For this PoC we decided to use CAN based only on the TED-XML XSD Schema
link:http://publications.europa.eu/mdr/resource/eprocurement/ted/R2.0.9/publication/latest/TED_EXPORT.xsd[R2.0.9.S01.E01 TED_EXPORT.xsd]
and the Contract Award Notice (CAN) form for Directive 2014 supporting the
link:http://publications.europa.eu/mdr/resource/eprocurement/ted/R2.0.9/publication/latest/F03_2014.xsd[F03_2014.xsd] standard form
(all schemas are published on the Publications Office (OP) link:http://publications.europa.eu/mdr/eprocurement/ted/index.html[MDR site].

For this PoC we downloaded the link:ftp://ted.europa.eu/monthly-packages/2018/[*.tar.gz] files corresponding
to January to May 2018. Bear in mind that, in the context of this PoC, we only extract data and import into the graph store the
CANs for Directive 2014. However the TED_EXPORT.xsd includes all the forms (F01 to  F25) and the extraction process is able to extract data
from many of these forms, as they share a large part of the elements (see "Activity 6: ETL process", just below). If you want a go with these
other forms just uncomment the line "#DOCUMENT_TYPE_ID=1,2,3,22,23,24,25 " and comment the line "DOCUMENT_TYPE_ID=3" in the `epo.properties` file.


=== Activity 6: ETL process

.ETL process development, activity summary
[cols="<1,<4"]
|===
|*Activity name*:|Develop ETL process.
|*Responsible team*:|OP's team.
|*Inputs*:|TED-XML schemas (on MDR) and TED notices published on the TED link:ftp://ted.europa.eu/[FTP] server.
|*Outputs*:|TED to ePO Mapping (Wiring), Java code, XSL-T architecture, other resources (available on the GitHub repository and accessible
via the GitHub Wiki page link
link:https://github.com/eprocurementontology/eprocurementontology/tree/master/v2.0.0/05_Implementation/epo-etl[Data Loading development (ETL)].
|===

==== TED to ePO Mapping
ETL stands for Extraction, Transformation and Loading. The first step (Extraction) requires to identify well where the data of origin are and
how they are expressed. For this, the ePO analysts produced a link:https://eprocurementontology.github.io/Mapping/Mapping%20TED%20XML%20to%20ePO.html[map]
putting side by side (wiring) each element of the TED-XML Schema (link:http://publications.europa.eu/mdr/eprocurement/ted/index.html[R2.0.9.S02.E01])
and the corresponding element in ePO.

=== Technical approach
The ETL process was developed based on two technologies:

. *Java*: version JDK 1.8 was used to build a Maven project (see link:https://github.com/eprocurementontology/eprocurementontology/blob/master/v2.0.0/05_Implementation/epo-etl/pom.xml[pom.xml]
configuration file). The output of the build process is a "*.war" file. The link:https://github.com/eprocurementontology/eprocurementontology/tree/master/v2.0.0/05_Implementation/epo-etl/main/java/epo[source code]
is available on the GitHub code repository. This java code is responsible for (i) organising the TED-XML files; (ii) launching the extraction + transformation and/or the
loading the data into the graph store, and (iii) log all the events and generate logs for monitoring the process;

. *XSL-T*: version XSL-T 3.0 was used to draft a set of link:https://github.com/eprocurementontology/eprocurementontology/tree/master/v2.0.0/05_Implementation/epo-etl/main/resources/xslt[stylesheets]
the mission of which is to read the TED-XML files (Extraction) and transform that information into
SPARQL INSERT patterns. Per each TED-XML a new TXT document is created with the mapped SPARQL INSERT patterns.
The name of the resulting TXT takes the name of the TED XML file and appends the suffix "_output.txt".
The piece of code below illustrates one of those examples (if you use the identifier of the document you
should be able to find the TED-XML source in the TED Portal).

.Result of transforming the TED-XML instance "091271-2018" into ePO-v2.00 SPARQL INSERT queries
[code]
----
PREFIX : <http://data.europa.eu/ePO/ontology#>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX skos: <http://www.w3.org/2004/02/skos/core#>
PREFIX org: <http://www.w3.org/ns/org#>
PREFIX vcard: <http://www.w3.org/2006/vcard/ns#>
PREFIX rov: <http://www.w3.org/ns/regorg#>
PREFIX ccts: <http://www.unece.org/cefact#>
PREFIX euvoc: <http://publications.europa.eu/ontology/euvoc#>
PREFIX ubl: <http://docs.oasis-open.org/ubl#>
PREFIX epo-rd: <http://data.europa.eu/ePO/referencedata#>

INSERT DATA
{
	Graph <http://data.europa.eu/ePO/ontology>{
		:CAN_091271-2018 rdf:type :ContractAwardNotice ;
			:hasPublicationDate "2018-03-01T00:00:00"^^xsd:dateTime ;
			:hasDocumentIdentifier :CAN_ID_091271-2018
	}
};
INSERT DATA
{
	Graph <http://data.europa.eu/ePO/ontology>{
		:CAN_ID_091271-2018 rdf:type ccts:Identifier ;
			ccts:identifierValue "091271-2018" ;
			ccts:schemeAgencyID "eu.europa.publicationsoffice.epo"
	}
}
...
<--1-->
----
<1> See link:https://github.com/eprocurementontology/eprocurementontology/blob/master/v2.0.0/05_Implementation/epo-etl/test/resources/output/SPARQL_Queries.zip[GitHub code repository]
or execute the code for complete examples.

.A note about the performance
[NOTE]
====
The Java code developed and the XSL-T approach are extremely fast:

* *Transformation speed*: _1 notice x 2 ms_. One Contract Award Notice transformed into a SPARQL file with multiple INSERT operations in
about 2 milliseconds. See the use of XMLStreamReader APIs (e.g. STAX) to capture the metadata about the TED-XML instances
in link:https://github.com/eprocurementontology/eprocurementontology/blob/master/v2.0.0/05_Implementation/epo-etl/src/main/java/epo/common/XSLTTransformer.java[XSLTTransformer.java].

* *Insertion speed*: _1 notice x 0,5 s_. One Contract Award Notice containing hundreds of INSERT operations inserted in the GraphDB as one single transaction
in about 0,5 seconds. When the file is greater than 1MB the INSERT operations are split into individual transactions, in
which case the operations can consume up to around 1 second. See java code in
link:https://github.com/eprocurementontology/eprocurementontology/blob/master/v2.0.0/05_Implementation/epo-etl/src/main/java/epo/common/KBManagement.java[KBManagement.java].
====

==== Code Execution

You can execute the code at least in two ways:

. Either you clone the project onto your machine, import the Maven project in your preferred Java editor tool and
execute the main class link:https://github.com/eprocurementontology/eprocurementontology/blob/master/v2.0.0/05_Implementation/epo-etl/main/java/epo/MainETLProcess.java[MainETLProcess].

. Alternatively you may unzip the *.war file and execute the compiled code from a console window.
The piece of code below provides a very simple script illustrating how this can be done:

.Launching the code, a simple bash shell script
[source,java]
----
#!/bin/bash

arg="$1"
exec java -classpath "lib/*:classes/." epo.MainETLProcess $arg

----

Beware that the MainETLProcess takes one argument:

.Acceptable arguments
[source]
----
Usage: epo.MainETLProcess [-t]|[-i]|[-a]

Valid arguments are:

-t .... transforms XML into .txt files containing the SPARQL queries, but does not execute the queries.
-i .... executes the SPARQL queries only.
-a .... does everything.

Options are mutually exclusive. Only one option is accepted.

Example:

 java -classpath "lib/*:classes/." epo.MainETLProcess -t
 java -classpath "lib/*:classes/." epo.MainETLProcess -i
 java -classpath "lib/*:classes/." epo.MainETLProcess -a
----

==== ETL execution configuration

The java code uses a file named _*epo.properties*_. This file is to be located under the `/home/user`
directory of the computer from where the code is executed. See below an example of how this configuration
file looks like. Notice the two lines about the proxy configuration.

.The _epo.properties_ file, example
[code]
----
#Thu Jun 28 10:49:40 CEST 2018

### Graph db access ###############################################################################
#GRAPH_STORE_URL=http://34.249.1.15:7200
GRAPH_STORE_URL=http://localhost:7200
GRAPH_STORE_USER=paulakeen
GRAPH_STORE_PASSWORD=shootingNicely2018Times
GRAPH_STORE_REPOSITORY=ePO_test

### Proxy configuration ##########################################################################
#PROXY_URL=10.110.8.42
#PROXY_PORT=8080

### Directories configuration ####################################################################
## The directory where the TED-XML files are located
INPUT_DATA_DIR=/TED-Resources
## The directory where the SPARQL INSERT TXT files, resulting form the XSL-T transformation, are written.
## This directory is the input directory from where the TXT files are taken to populate the Graph Store.
OUTPUT_DATA_DIR=/TED-OUTPUT
## The directory where the java application logs the operations executed and execeptions.
LOG_DATA_DIR=/TED-LOG
## Where the XSL-T architecture files are located. Relative or absolute paths can be specified.
## Relative paths are relative to the path from where the etl-process is launched.
TED_TO_EPO_XSL=./src/main/resources/xslt/TEDXSD_to_ePOTTL.xsl
## Where the TED XSD Schemas are located. Relative or absolute paths can be specified.
## Relative paths are relative to the path from where the etl-process is launched.
## @DEPRECATED comment="the latest version uses STAX XMLStreamReader and works on multiple TED_XSD_VERSIONS
TED_EXPORT_XSD=./src/main/resources/TED_publication_R2.0.9.S02.E01_003-20170123/TED_EXPORT.xsd
## Subystem IDs, XSD root element local name of the Subsystems that produced the XML instances that are
## requested to be processed. A comma separated list of names is expected.
TED_SUBSYSTEMS=TED_EXPORT
## Version IDs of the TED-XSD schemas upon which the XML that are requested to be
## processed are instantiated. A comma separated list of names is expected.
#TED_XSD_VERSIONS=R2.0.9.S02.E01, R2.0.9.S01.E01
TED_XSD_VERSIONS=R2.0.9.S02.E01
## Form types requested to be  processed.
#TED_XSD_FORM_TYPES=F01, F02, F03
TED_XSD_FORM_TYPES=F03
----

NOTE: Notice that each execution of the ETL process generates a log file in the specified directory (property "LOG_DATA_DIR").
The log files append the total number of files transformed and inserted at the end of the file. These figures can be used
to study the amount and types of documents that have been published by the OP. For an example see the section following
link:#activity-7-populate-graph-store[Activity 7: Populate Graph store].
The data were extracted from the logs about the transformation of each month of 2018, separately, from January to May.

=== Activity 7: Populate Graph store

.Populate the Graph store, activity summary
[cols="<1,<4"]
|===
|*Activity name*:|Populate the Graph store.
|*Responsible team*:|OP's team.
|*Inputs*:|The result of the XSL-T-based transformation (SPARQL INSERT queries).
|*Outputs*:|The link:34.249.1.15:7200[Graph store] is populated with triples.
|===

A large amount of TXT files containing the SPARQL INSERT queries was automatically obtained - out of the
transformation- for the five first months of 2018. The table and bar graphic below show the exact number of
files processed and the number of Contract Award Notices imported into the Graph Store.

The Graph Store chosen for this PoC was the Community version of GraphDB (version 8.5) which can be freely
downloaded from the link:https://ontotext.com/[Ontotext] website.

.Total of Notices and number of Contract Award Notices used to populate the Graph store
image::Statistics-2018.png[Number of Notices, align="center"]

.Frequency of Notices
image::Statistics-Frequency-2018.png[Frequency of Notices, align="center"]

=== Activity 8: SPARQL Queries

.Develop SPARQL Queries, activity summary
[cols="<1,<4"]
|===
|*Activity name*:|Develop SPARQL Queries.
|*Responsible team*:|OP's team.
|*Inputs*:|Competency Questions (https://eprocurementontology.github.io/Competency_questions/SPARQL_examples.html[QCs examples]).
|*Outputs*:|The link:34.249.1.15:7200[Graph store] is populated with triples.
|===

The document link:https://eprocurementontology.github.io/Competency_questions/SPARQL_examples.html[SPARQL Query examples]
provides a few examples that were provided for the Working Group (WG) members to have a glimpse
at how efficiently the ePO is responding.

==== Query examples

*Example 1*: One very first exercise would consist in checking the amount of Contract Award Notice and to compare it to the number of
transformations executed and compiled in the log files. For this open a browser, introduce the URL or IP of the GraphDB server
(e.g. 34.249.1.15:7200) and copy this SPARQL Query in the textfield of the SPARQL Endpoint:

.Counting the number of Contract Award Notices
[code]
----
PREFIX : <http://data.europa.eu/ePO/ontology#>
select ?s (count(?did) as ?cdid) where {
	?s a :ContractAwardNotice;
    	:hasDocumentIdentifier ?did;
} group by ?s
----
.Number of Contract Award Notices between the 1s. January and the 30th May 2018
image::CountingCANs.png[Counting CANs, align="center"]

*Example 2*: List all the winners, the size of the company and the date of award.

.Winners, size of the company, date of the awarding
[code]
----
PREFIX : <http://data.europa.eu/ePO/ontology#>
PREFIX rov: <http://www.w3.org/ns/regorg#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
select distinct ?Winner_Name ?WinnerSize ?Awarded_Date where {
                ?Award_Result :hasWinner ?Winner ;
                :hasAwardResultDateOfConclusion ?Awarded_Date .
    			?Winner :usesEOIndustryClassificationType ?WinnerSize ;
            	rov:legalName  ?Winner_Name
}
----

.Winners
image::QueryExample1.png[Winners, size and award date, align="center"]

*Example 3*: Number of contracts awarded for each CPV (beware that one Contract Award Notice may
refer to multiple contracts).

.Number of contracts per CPV
[code]
----
PREFIX : <http://data.europa.eu/ePO/ontology#>
SELECT ?cpv (COUNT(DISTINCT(?contract)) AS ?number_contracts) where {
                ?contract a :Contract;
        		:hasContractPurpose ?purpose.
    			?purpose :hasCPVType ?cpv.
} group by ?cpv order by desc(?number_contracts)
----

.Number of contracts per CPV
image::QueryExample3.png[Contracts per CPV, align="center"]


See the document link:https://eprocurementontology.github.io/Competency_questions/SPARQL_examples.html[SPARQL Query examples]
for more contextualisation and examples.

=== Activity 9: Test and debug

.Test and debug, activity summary
[cols="<1,<4"]
|===
|*Activity name*:|Prepare/Execute/Debug Test Reports
|*Responsible team*:|OP's team.
|*Inputs*:|SPARQL queries
|*Outputs*:|SPARQL table results
|===



=== Activity 10: Validate results

.Validate results, activity summary
[cols="<1,<4"]
|===
|*Activity name*:|Use SPARQL queries, validate results.
|*Responsible team*:|Working Group WG) members.
|*Inputs*:|Example SPARQL queries supplied by the OP's team.
|*Outputs*:|SPARQL result-tables.
|===

(TODO): WG to prepare their own Queries.

=== Activity 11: Provide feedback
.Provide feedback, activity summary
[cols="<1,<4"]
|===
|*Activity name*:|Provide feedback
|*Responsible team*:|Working Group WG) members.
|*Inputs*:|OP's example queries and WG's own Competency Questions and queries
|*Outputs*:|Feed-back via the link:https://github.com/eprocurementontology/eprocurementontology/issues[GitHub Issues]
work-space.
|===